{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKEwMM2Mx4vh"
      },
      "source": [
        "apikey= 'SEDcFR325HFEwsxuz'\n",
        "apisecretkey= 'SEDcFR325HFEwsxuzSEDcFR325HFEwsxuzSEDcFR325HFEwsxuz'\n",
        "accesstoken='126SEDcFR325HFEwsxuzbLHXKSEDcFR325HFEwsxuz6m56xlvGSEDcFR325HFEwsxuz'\n",
        "accesstokensecret='JS1SEDcFR325HFEwsxuzTCsX6g8JpLzFA5SEDcFR325HFEwsxuzD'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tweepy as tw\n",
        "\n",
        "auth = tw.OAuthHandler(apikey, apisecretkey)\n",
        "auth.set_access_token(accesstoken, accesstokensecret)\n",
        "\n",
        "api = tw.API(auth,wait_on_rate_limit=True)"
      ],
      "metadata": {
        "id": "rW-BXFpslTRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def extract_timeline_as_df(timeline_list):\n",
        "    columns = set()\n",
        "    allowed_types = [str, int]\n",
        "    tweets_data = []\n",
        "    for status in timeline_list:\n",
        "        status_dict = dict(vars(status))\n",
        "        keys = status_dict.keys()\n",
        "        single_tweet_data = {\"user\": status.user.screen_name, \"author\": status.author.screen_name}\n",
        "        for k in keys:\n",
        "            try:\n",
        "                v_type = type(status_dict[k])\n",
        "            except:\n",
        "                v_type = None\n",
        "            if v_type != None:\n",
        "                if v_type in allowed_types:\n",
        "                    single_tweet_data[k] = status_dict[k]\n",
        "                    columns.add(k)\n",
        "        tweets_data.append(single_tweet_data)\n",
        "\n",
        "\n",
        "    header_cols = list(columns)\n",
        "    header_cols.append(\"user\")\n",
        "    header_cols.append('author')\n",
        "    df = pd.DataFrame(tweets_data, columns=header_cols)\n",
        "    return df"
      ],
      "metadata": {
        "id": "Q93ZKEOslTUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1: What's on MY timeline\n",
        "my_timeline = api.home_timeline()\n",
        "df2 = extract_timeline_as_df(my_timeline)\n",
        "df2.head(3)"
      ],
      "metadata": {
        "id": "-XOR-zB1lTXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2: What's on different user's timeline\n",
        "user = api.get_user(\"JioMart\")#JioMart\n",
        "user_timeline = user.timeline()\n",
        "df3 = extract_timeline_as_df(user_timeline)\n",
        "df3.head(2)"
      ],
      "metadata": {
        "id": "Z_iWdkyFlTdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Find all followers and friends count\n",
        "user = api.get_user(\"JioMart\")\n",
        "print(user.followers_count, user.friends_count)"
      ],
      "metadata": {
        "id": "2OU5BIB9lTfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 #following\n",
        "user_friends = user.friends()\n",
        "for friend in user_friends:\n",
        "  print(friend.screen_name)"
      ],
      "metadata": {
        "id": "MbgX6mH1lTi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5: My \"Tweets & replies\"\n",
        "import tweepy\n",
        "for i, status in enumerate(tweepy.Cursor(api.home_timeline, count=5).items(4)):\n",
        "    print(i, status.text)"
      ],
      "metadata": {
        "id": "ptfv2FaBxHn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6_1 - Retrieve Tweets\n",
        "from textblob import TextBlob\n",
        "public_tweets = api.search('JioMart')\n",
        "\n",
        "for tweet in public_tweets:\n",
        "    print(tweet.text)\n",
        "    \n",
        "    #Step 4 Perform Sentiment Analysis on Tweets\n",
        "    analysis = TextBlob(tweet.text)\n",
        "    print(analysis.sentiment)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "OBUOSkt1hd4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6_1: Track hashtag from any date\n",
        "!pip install -q snscrape\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd '/content/drive/My Drive/Colab Notebooks/POC_Projects/'\n",
        "\n",
        "from datetime import date, timedelta\n",
        "today = date.today()\n",
        "end_date = today\n",
        "\n",
        "search_term = 'JioMart'\n",
        "from_date = date.today() - timedelta(days=10)# '2022-08-28'\n",
        "\n",
        "os.system(f\"snscrape --since {from_date} twitter-search '{search_term} until:{end_date}' > Date_Range_tweets.txt\") #will take more than 10 minutes\n",
        "if os.stat(\"Date_Range_tweets.txt\").st_size == 0:\n",
        "  counter = 0\n",
        "else:\n",
        "  df = pd.read_csv('Date_Range_tweets.txt', names=['link'])\n",
        "  counter = df.size\n",
        "\n",
        "print('Number Of Tweets : '+ str(counter))"
      ],
      "metadata": {
        "id": "Rpw8xFbVheGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6_1_Extracting Exact Tweeets -- may take 4 to 5 mins\n",
        "max_results = 5383 #add the number as per ABOVE 'Number Of Tweets'\n",
        "extracted_tweets = \"snscrape --format '{content!r}'\"+ f\" --max-results {max_results} --since {from_date} twitter-search '{search_term} until:{end_date}' > Date_Range_tweets.txt\"\n",
        "\n",
        "# extracted_tweets = (re.sub('@[a-z,_,A-Z,0-9,:]+ ', '', i) for i in extracted_tweets)\n",
        "\n",
        "os.system(extracted_tweets)\n",
        "if os.stat(\"Date_Range_tweets.txt\").st_size == 0:\n",
        "  print('No Tweets found')\n",
        "else:\n",
        "  df = pd.read_csv('Date_Range_tweets.txt', names=['content'])\n",
        "  for row in df['content'].iteritems():\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "CDXq2IO6-hhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6: Track hashtag from any date\n",
        "search_words = [\"#JioMart\"]\n",
        "date_since = \"2022-11-01\"\n",
        "\n",
        "tweets = tw.Cursor(api.search,\n",
        "              q=search_words,\n",
        "              lang=\"en\",\n",
        "              since=date_since).items() # limit the count using e.g. .items(5)\n",
        "\n",
        "tweets\n",
        "\n",
        "tweet_details = [[tweet.geo, tweet.text,tweet.user.screen_name, tweet.user.location,tweet.created_at,[e['text'] for e in tweet._json['entities']['hashtags']], tweet.user.followers_count] for tweet in tweets]\n",
        "# tweet_details\n",
        "\n",
        "import pandas as pd\n",
        "tweet_df = pd.DataFrame(data=tweet_details, columns=['geo','text','user', \"location\",\"Timestamp\",\"All_hashtags\",\"Followers_Count\"])\n",
        "\n",
        "pd.set_option('max_colwidth', 80)\n",
        "tweet_df.head(5)"
      ],
      "metadata": {
        "id": "TUPZBcXVxHq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7: User's count who tweeted\n",
        "tweet_df.user.value_counts()"
      ],
      "metadata": {
        "id": "CqaomqGu8pPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8: Tweeted from location\n",
        "tweet_df.location.value_counts()"
      ],
      "metadata": {
        "id": "T6WC431D8pSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9: Save csv file after data cleaning\n",
        "import re\n",
        "def clean_tweets(text):\n",
        "    text = re.sub(\"RT @[\\w]*:\",\"\",text)\n",
        "    text = re.sub(\"@[\\w]*\",\"\",text)\n",
        "    text = re.sub(\"https?://[A-Za-z0-9./]*\",\"\",text)\n",
        "    text = re.sub(\"\\n\",\"\",text)\n",
        "\n",
        "\n",
        "    text = re.sub(\"\\$\\w*\",\"\",text)\n",
        "    text = re.sub(\"^RT[\\s]+\",\"\",text)\n",
        "    text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\",\"\",text)\n",
        "    text = re.sub(\"#\",\"\",text)\n",
        "\n",
        "    return text\n",
        "\n",
        "tweet_df['text']=tweet_df['text'].apply(lambda x: clean_tweets(x))\n",
        "\n",
        "# tweet_df.head(20)\n",
        "tweet_df.to_csv('Hashed_tweets.csv') #check file in right side folder\n",
        "# !ls"
      ],
      "metadata": {
        "id": "1FPq747bAjj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10 SENTIMENT ANALYSIS\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "tweet_df['text'].apply(lambda x: [print(\"\\tText : {}, Entity : {}\".format(ent.text, ent.label_)) if (not ent.text.startswith('#')) else \"\"  for ent in nlp(x).ents])\n"
      ],
      "metadata": {
        "id": "rr-RBLPmAjtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tweet_df['entities']=tweet_df['text'].apply(lambda x: [(ent.text, ent.label_) if (not ent.text.startswith('#')) else \"\" for ent in nlp(x).ents])\n",
        "print(tweet_df.head(5))"
      ],
      "metadata": {
        "id": "Xhlb3htKDRuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7: NER\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "def ner_tweets(text):\n",
        "  ne_token =word_tokenize(text)\n",
        "  ne_tags = nltk.pos_tag(ne_token)\n",
        "\n",
        "  ne_ner = ne_chunk(ne_tags)\n",
        "  return ne_ner"
      ],
      "metadata": {
        "id": "ku0walhBDR8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "tweet_df['sentiment']=tweet_df['text'].apply(lambda x: sid.polarity_scores(x))\n",
        "tweet_df['NER_1']=tweet_df['text'].apply(lambda x: [(ent.text,ent.start_char, ent.end_char, ent.label_) if (not ent.text.startswith('#')) else \"\" for ent in nlp(x).ents])\n",
        "tweet_df['NER_2']=tweet_df['text'].apply(lambda x: ner_tweets(x))\n",
        "\n",
        "\n",
        "tweet_df.to_csv('FINAL.csv')\n",
        "tweet_df.head(10) #sentiment is added"
      ],
      "metadata": {
        "id": "VnnV8n2yAj10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11: Export csv file as per inputted hashtag\n",
        "import json\n",
        "import csv\n",
        "import tweepy\n",
        "def search_for_hashtags(apikey, apisecretkey, accesstoken, accesstokensecret, hashtag_phrase):\n",
        "    #create authentication for accessing Twitter\n",
        "    auth = tweepy.OAuthHandler(apikey, apisecretkey)\n",
        "    auth.set_access_token(accesstoken, accesstokensecret)\n",
        "\n",
        "    #initialize Tweepy API\n",
        "    api = tweepy.API(auth)\n",
        "    \n",
        "    #get the name of the spreadsheet we will write to\n",
        "    fname = ''.join(re.findall(r\"#(\\w+)\", hashtag_phrase))\n",
        "\n",
        "    #open the spreadsheet we will write to\n",
        "    with open('%s 2_tweets.csv' % (fname), 'w') as file:\n",
        "\n",
        "      w = csv.writer(file)\n",
        "\n",
        "      #write header row to spreadsheet\n",
        "      w.writerow(['timestamp', 'tweet_text', 'username', 'all_hashtags', 'followers_count'])\n",
        "\n",
        "      #for each tweet matching our hashtags, write relevant info to the spreadsheet\n",
        "      for tweet in tweepy.Cursor(api.search, q=hashtag_phrase+' -filter:retweets', lang=\"en\", tweet_mode='extended').items(10):\n",
        "        w.writerow([tweet.created_at, tweet.full_text.replace('\\n',' ').encode('utf-8'), tweet.user.screen_name.encode('utf-8'), [e['text'] for e in tweet._json['entities']['hashtags']], tweet.user.followers_count])\n",
        "            \n",
        "\n",
        "\n",
        "# hashtag_phrase = input('Hashtag Phrase: ') ##FIFAWorldCup\n",
        "hashtag_phrase = 'FIFAWorldCup' ##FIFAWorldCup\n",
        "if __name__ == '__main__':\n",
        "  search_for_hashtags(apikey, apisecretkey, accesstoken, accesstokensecret, hashtag_phrase)"
      ],
      "metadata": {
        "id": "uavowgPkgcqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12: Reply any tweet\n",
        "# og_tweet = api.get_status(\"1584907651753746432\")\n",
        "# # print(og_tweet.user.screen_name, og_tweet.id)\n",
        "# my_reply = api.update_status(f\"@{og_tweet.user.screen_name} Wow this cool!\", og_tweet.id)\n",
        "#print(my_reply.id, my_reply.user.screen_name)"
      ],
      "metadata": {
        "id": "1ZaANxeqeLsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13: Word Cloud\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt \n",
        "text = \" \".join(review for review in tweet_df.text)\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(max_font_size=50, max_words=1000,background_color=\"white\",contour_width=3, contour_color='firebrick').generate(text)\n",
        "\n",
        "# Display the generated image:\n",
        "# the matplotlib way:\n",
        "plt.figure(figsize=[20,10])\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# store to file\n",
        "plt.savefig(\"WC.png\", format=\"png\")"
      ],
      "metadata": {
        "id": "QY8GSYNvAj93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bb-aaFx0HYpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "in7DYoviHYxG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}